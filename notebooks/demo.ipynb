{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ce6d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_google-genai\n",
    "#!pip install tiktoken\n",
    "#!pip install pypdf\n",
    "#!pip install langchain langchain-community\n",
    "#!pip install openai\n",
    "#!pip install langchain-openai\n",
    "#!pip install python-dotenv\n",
    "#!pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cca8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\", \"..\\\\RAG Project Dataset\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"rag-qna\")\n",
    "PINECONE_NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\", \"default\")\n",
    "PINECONE_METRIC = os.getenv(\"PINECONE_METRIC\", \"cosine\")\n",
    "\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 300\n",
    "RETRIEVER_K = 10\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a research assistant answering questions based on academic papers.\n",
    "\n",
    "Use ONLY the information from the provided context.\n",
    "You may combine information from multiple context passages.\n",
    "\n",
    "When answering:\n",
    "- Be concise and technically precise.\n",
    "- Avoid overgeneralization beyond what is stated in the papers.\n",
    "- Cite ONLY the most relevant source passages (maximum 3).\n",
    "\n",
    "If the answer is not supported by the context, say:\n",
    "\"I could not find sufficient information in the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c2d411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdfs(data_dir: str):\n",
    "    data_path = Path(data_dir)\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"DATA_DIR not found: {data_path}\")\n",
    "\n",
    "    pdf_paths = sorted(p for p in data_path.rglob(\"*.pdf\"))\n",
    "    if not pdf_paths:\n",
    "        raise FileNotFoundError(f\"No PDFs found in: {data_path}\")\n",
    "\n",
    "    docs = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "docs = load_pdfs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ba9d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\RAG Project Dataset\\1706.03762v7.pdf: 22 chunks\n",
      "..\\RAG Project Dataset\\2005.11401v4.pdf: 35 chunks\n",
      "..\\RAG Project Dataset\\2005.14165v4.pdf: 127 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    counts = {}\n",
    "    for chunk in chunks:\n",
    "        src = chunk.metadata.get(\"source\", \"unknown\")\n",
    "        counts[src] = counts.get(src, 0) + 1\n",
    "    for src, count in sorted(counts.items()):\n",
    "        print(f\"{src}: {count} chunks\")\n",
    "    return chunks\n",
    "chunked_docs = chunk_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b0190bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding provider: openai\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "\n",
    "# Support multiple embedding providers via the EMBEDDING_PROVIDER env var\n",
    "# Options: \"google\" (default), \"openai\", \"hf\" (Hugging Face)\n",
    "env_provider = getenv(\"EMBEDDING_PROVIDER\")\n",
    "if env_provider:\n",
    "    provider = env_provider.lower()\n",
    "else:\n",
    "    # prefer openai automatically if OPENAI_API_KEY exists\n",
    "    provider = \"openai\" if getenv(\"OPENAI_API_KEY\") else \"google\"\n",
    "\n",
    "if provider == \"google\":\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "elif provider == \"openai\":\n",
    "    # OpenAI embeddings are in the langchain_openai package\n",
    "    try:\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"OpenAIEmbeddings not found. Install langchain_openai with: pip install langchain-openai\"\n",
    "        ) from e\n",
    "    # Use 1024 dimensions to match the Pinecone index\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported EMBEDDING_PROVIDER: {provider}\")\n",
    "\n",
    "print(f\"Using embedding provider: {provider}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12f3e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Create Pinecone vector store with error handling for embedding/quota issues\n",
    "\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "        documents=chunked_docs,\n",
    "        embedding=embeddings,\n",
    "        index_name=PINECONE_INDEX_NAME,\n",
    "        namespace=PINECONE_NAMESPACE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc92fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ba6bb270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Positional encoding in Transformers is implemented using sinusoidal functions or learned embeddings to provide information about the position of tokens in the input sequence. This is necessary because the Transformer architecture does not have a built-in notion of sequence order, as it relies entirely on self-attention mechanisms rather than recurrent or convolutional structures that inherently process sequences in order.\n",
      "\n",
      "The sinusoidal positional encoding allows the model to capture the relative positions of tokens, enabling it to differentiate between sequences that contain the same tokens but in different orders. This encoding is added to the input embeddings to ensure that the model can utilize the positional information when processing the sequence.\n",
      "\n",
      "In the context of the Transformer, the authors note that they initially used sinusoidal positional encoding but also experimented with learned positional embeddings, finding that both approaches yielded similar results in terms of model performance (Section 6.3). \n",
      "\n",
      "Thus, positional encoding is crucial for maintaining the sequential nature of the input data, allowing the model to understand the order of tokens effectively.\n",
      "\n",
      "Sources:\n",
      "- 1706.03762v7.pdf (Page 2.0)\n",
      "- 1706.03762v7.pdf (Page 1.0)\n",
      "- 1706.03762v7.pdf (Page 1.0)\n",
      "- 1706.03762v7.pdf (Page 4.0)\n",
      "- 1706.03762v7.pdf (Page 8.0)\n",
      "- 1706.03762v7.pdf (Page 0.0)\n",
      "- 1706.03762v7.pdf (Page 8.0)\n",
      "- 1706.03762v7.pdf (Page 9.0)\n",
      "- 1706.03762v7.pdf (Page 9.0)\n",
      "- 1706.03762v7.pdf (Page 7.0)\n"
     ]
    }
   ],
   "source": [
    "# Build and run the QA chain using LCEL with OpenAI\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE,\n",
    "    input_variables=['context', 'question'],\n",
    ")\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': RETRIEVER_K})\n",
    "\n",
    "# Format retrieved documents into context\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build chain\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run query\n",
    "question = \"Explain how positional encoding is implemented in Transformers and why it is necessary.\"\n",
    "answer = chain.invoke(question)\n",
    "\n",
    "# Get source documents\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "sources = [\n",
    "    {'document': doc.metadata.get('source'), 'page': doc.metadata.get('page')}\n",
    "    for doc in retrieved_docs\n",
    "]\n",
    "\n",
    "print('Answer:', answer)\n",
    "print('\\nSources:')\n",
    "for s in sources:\n",
    "    name = os.path.basename(s['document'] or '')\n",
    "    page = s['page']\n",
    "    if page is not None:\n",
    "        print(f'- {name} (Page {page})')\n",
    "    else:\n",
    "        print(f'- {name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
